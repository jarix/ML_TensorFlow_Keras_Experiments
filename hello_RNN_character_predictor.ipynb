{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello RNN next character predictor.\n",
    "\n",
    "RNN trained with Shakespeare's work to predict next character in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version:  3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)]\n",
      "Numpy Version:  2.0.2\n",
      "Pandas Version:  2.2.3\n",
      "TensorFlow Version:  2.18.0\n"
     ]
    }
   ],
   "source": [
    "# Prerequisites\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(\"Python Version: \", sys.version)\n",
    "print(\"Numpy Version: \", np.__version__)\n",
    "print(\"Pandas Version: \", pd.__version__)\n",
    "print(\"TensorFlow Version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data (Shakespeare's works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", input_data_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394\n",
      "Begins with:\n",
      " First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of text:\" , len(shakespeare_text))\n",
    "print(\"Begins with:\\n\", shakespeare_text[:200] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to lowercase and Encode characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters:  \n",
      " !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\n",
      "Number of characters:  39\n"
     ]
    }
   ],
   "source": [
    "# Show all 39 characters (lower case)\n",
    "chars = \"\".join(sorted(set(shakespeare_text.lower())))\n",
    "print(\"Characters: \", chars)\n",
    "print(\"Number of characters: \", len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\", standardize=\"lower\")\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "encoded = text_vec_layer([shakespeare_text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens:  39\n",
      "Dataset size:  1115394\n"
     ]
    }
   ],
   "source": [
    "# Drop padding (0) and unknown (1) tokens\n",
    "encoded -= 2\n",
    "# Number of tokens\n",
    "nr_tokens = text_vec_layer.vocabulary_size() - 2\n",
    "print(\"Number of tokens: \", nr_tokens )\n",
    "ds_size = len(encoded)\n",
    "print(\"Dataset size: \", ds_size )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to convert sequence of IDs to inputs/targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(100_000, seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[ 4,  5,  2, 23]])>, <tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[ 5,  2, 23,  3]])>)]\n"
     ]
    }
   ],
   "source": [
    "# try to_dataset()\n",
    "ds_sample = list(to_dataset(text_vec_layer([\"To be\"])[0], length=4))\n",
    "print(ds_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into test, validation, and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100\n",
    "tf.random.set_seed(42)\n",
    "ds_train = to_dataset(encoded[:1_000_000], length=length, shuffle=True, seed=42)\n",
    "ds_val = to_dataset(encoded[1_000_000:1_060_000], length=length)\n",
    "ds_test = to_dataset(encoded[1_060_000:], length=length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train the model\n",
    "\n",
    "NOTE:  Need GPU to train in a reasonable time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42) \n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(input_dim=nr_tokens, output_dim=16),  # Embed the character IDs\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.Dense(nr_tokens, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "cb_model_ckpt = keras.callbacks.ModelCheckpoint(\"my_shakespeare_model.keras\", \n",
    "                                                monitor=\"val_accuracy\", save_best_only=True)\n",
    "history = model.fit(ds_train, validation_data=ds_val, epochs=10, callbacks=[cb_model_ckpt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap with preprocessing \n",
    "model_shakespeare = keras.Sequential([\n",
    "    text_vec_layer, \n",
    "    keras.layers.Lambda(lambda X: X - 2),  # skip <PAD> or <UNK> tokens\n",
    "    model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = model_shakespeare.predict([\"To be or not to b\"])[0, -1]\n",
    "y_pred = tf.argmax(y_proba)  # Pick the most probable character\n",
    "text_vec_layer.get_vocabulary()[y_pred + 2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
